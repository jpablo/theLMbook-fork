{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d22e663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T04:51:51.122866Z",
     "start_time": "2025-04-16T04:51:51.118224Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e4cae34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T04:51:56.226174Z",
     "start_time": "2025-04-16T04:51:56.222779Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_vocabulary(corpus: list[str]):\n",
    "    vocabulary: defaultdict[str, int] = defaultdict(int)\n",
    "    charset: set[str] = set()\n",
    "    for word in corpus:\n",
    "        word_with_marker = '_' + word\n",
    "        characters = list(word_with_marker)\n",
    "        charset.update(characters)\n",
    "        tokenized_word = ' '.join(characters)\n",
    "        vocabulary[tokenized_word] += 1\n",
    "    return vocabulary, charset\n",
    "\n",
    "\n",
    "def get_pair_counts(vocabulary: dict[str, int]):\n",
    "    # Map[(String, String), Int]\n",
    "    pair_counts: defaultdict[tuple[str, str], int] = defaultdict(int)\n",
    "    for tokenized_word, count in vocabulary.items():\n",
    "        tokens = tokenized_word.split()\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            pair_counts[pair] += count\n",
    "    return pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938d9308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T04:52:02.112312Z",
     "start_time": "2025-04-16T04:52:02.109515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: defaultdict(<class 'int'>, {'_ M o v i e s': 1, '_ a r e': 1, '_ f u n': 1, '_ f o r': 1, '_ e v e r y o n e': 1, '_ e v e r y': 1, '_ t i m e': 1, '_ o n e': 1})\n",
      "charset: {'e', 'M', '_', 'v', 'i', 'a', 'f', 's', 'r', 'o', 'n', 'y', 't', 'm', 'u'}\n",
      "----- get_pair_counts(vocabulary) ----\n",
      "(('_', 'M'), 1)\n",
      "(('M', 'o'), 1)\n",
      "(('o', 'v'), 1)\n",
      "(('v', 'i'), 1)\n",
      "(('i', 'e'), 1)\n",
      "(('e', 's'), 1)\n",
      "(('_', 'a'), 1)\n",
      "(('a', 'r'), 1)\n",
      "(('r', 'e'), 1)\n",
      "(('_', 'f'), 2)\n",
      "(('f', 'u'), 1)\n",
      "(('u', 'n'), 1)\n",
      "(('f', 'o'), 1)\n",
      "(('o', 'r'), 1)\n",
      "(('_', 'e'), 2)\n",
      "(('e', 'v'), 2)\n",
      "(('v', 'e'), 2)\n",
      "(('e', 'r'), 2)\n",
      "(('r', 'y'), 2)\n",
      "(('y', 'o'), 1)\n",
      "(('o', 'n'), 2)\n",
      "(('n', 'e'), 2)\n",
      "(('_', 't'), 1)\n",
      "(('t', 'i'), 1)\n",
      "(('i', 'm'), 1)\n",
      "(('m', 'e'), 1)\n",
      "(('_', 'o'), 1)\n"
     ]
    }
   ],
   "source": [
    "vocabulary, charset = initialize_vocabulary(corpus=[\"Movies\", \"are\", \"fun\", \"for\", \"everyone\", \"every\", \"time\", \"one\"])\n",
    "print(\"vocabulary:\", vocabulary)\n",
    "print(\"charset:\", charset)\n",
    "print(\"----- get_pair_counts(vocabulary) ----\")\n",
    "for t in get_pair_counts(vocabulary).items(): print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4221193c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T04:52:05.509708Z",
     "start_time": "2025-04-16T04:52:05.506759Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_pair(vocabulary: defaultdict[str, int], pair: tuple[str, str]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Merges a given pair of symbols in the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        vocab (dict): A dictionary where keys are tokenized words and values are their frequencies.\n",
    "        pair (tuple): A tuple containing the pair of symbols to merge.\n",
    "    Returns:\n",
    "        dict: Updated vocabulary with the pair merged.\n",
    "    \"\"\"\n",
    "    new_vocabulary: dict[str, int] = {}\n",
    "    # ('o', 'v') => 'o\\\\ v'\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    # re.compile(r'(?<!\\S)o\\ v(?!\\S)', re.UNICODE)\n",
    "    # matches only whole token pairs\n",
    "    pattern = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
    "    # same as:\n",
    "    #  `vocabulary.map((tokenized_word, count) => merged(tokenized_word) -> count)`\n",
    "    #\n",
    "    for tokenized_word, count in vocabulary.items():\n",
    "        # in tokenized_word, replace ' o v ' ~> ' ov '\n",
    "        # '_ M o v i e s' ~> '_ M ov i e s'\n",
    "        new_tokenized_word = pattern.sub(\"\".join(pair), tokenized_word)\n",
    "        new_vocabulary[new_tokenized_word] = count\n",
    "    return new_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f796693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2b69415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T02:11:48.117238Z",
     "start_time": "2025-04-16T02:11:48.114985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ M o v i e s ~~> _ M ov i e s\n",
      "{'_ M ov i e s': 1, '_ a r e': 1, '_ f u n': 1, '_ f o r': 1, '_ e v e r y o n e': 1, '_ e v e r y': 1, '_ t i m e': 1, '_ o n e': 1}\n"
     ]
    }
   ],
   "source": [
    "print(merge_pair(vocabulary, ('o', 'v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1c5f1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T04:52:10.580883Z",
     "start_time": "2025-04-16T04:52:10.577781Z"
    }
   },
   "outputs": [],
   "source": [
    "def byte_pair_encoding(corpus: list[str], vocab_size: int):\n",
    "    vocabulary, charset = initialize_vocabulary(corpus)\n",
    "    merges = []\n",
    "    # start with a copy of charset\n",
    "    tokens = set(charset)\n",
    "    while len(tokens) < vocab_size:\n",
    "        pair_counts = get_pair_counts(vocabulary)\n",
    "        if not pair_counts:\n",
    "            break\n",
    "        most_frequent_pair: tuple[str, str] = max(pair_counts, key=pair_counts.get)\n",
    "        merges.append(most_frequent_pair)\n",
    "        vocabulary = merge_pair(vocabulary, most_frequent_pair)\n",
    "        new_token = ''.join(most_frequent_pair)\n",
    "        tokens.add(new_token)\n",
    "    return vocabulary, merges, charset, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "532a4ae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T04:52:13.993132Z",
     "start_time": "2025-04-16T04:52:13.987331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'_Movies': 1,\n",
       "  '_are': 1,\n",
       "  '_fun': 1,\n",
       "  '_for': 1,\n",
       "  '_everyone': 1,\n",
       "  '_every': 1,\n",
       "  '_time': 1,\n",
       "  '_one': 1},\n",
       " [('_', 'f'),\n",
       "  ('_', 'e'),\n",
       "  ('_e', 'v'),\n",
       "  ('_ev', 'e'),\n",
       "  ('_eve', 'r'),\n",
       "  ('_ever', 'y'),\n",
       "  ('o', 'n'),\n",
       "  ('on', 'e'),\n",
       "  ('_', 'M'),\n",
       "  ('_M', 'o'),\n",
       "  ('_Mo', 'v'),\n",
       "  ('_Mov', 'i'),\n",
       "  ('_Movi', 'e'),\n",
       "  ('_Movie', 's'),\n",
       "  ('_', 'a'),\n",
       "  ('_a', 'r'),\n",
       "  ('_ar', 'e'),\n",
       "  ('_f', 'u'),\n",
       "  ('_fu', 'n'),\n",
       "  ('_f', 'o'),\n",
       "  ('_fo', 'r'),\n",
       "  ('_every', 'one'),\n",
       "  ('_', 't'),\n",
       "  ('_t', 'i'),\n",
       "  ('_ti', 'm'),\n",
       "  ('_tim', 'e'),\n",
       "  ('_', 'one')],\n",
       " {'M', '_', 'a', 'e', 'f', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', 'v', 'y'},\n",
       " {'M',\n",
       "  '_',\n",
       "  '_M',\n",
       "  '_Mo',\n",
       "  '_Mov',\n",
       "  '_Movi',\n",
       "  '_Movie',\n",
       "  '_Movies',\n",
       "  '_a',\n",
       "  '_ar',\n",
       "  '_are',\n",
       "  '_e',\n",
       "  '_ev',\n",
       "  '_eve',\n",
       "  '_ever',\n",
       "  '_every',\n",
       "  '_everyone',\n",
       "  '_f',\n",
       "  '_fo',\n",
       "  '_for',\n",
       "  '_fu',\n",
       "  '_fun',\n",
       "  '_one',\n",
       "  '_t',\n",
       "  '_ti',\n",
       "  '_tim',\n",
       "  '_time',\n",
       "  'a',\n",
       "  'e',\n",
       "  'f',\n",
       "  'i',\n",
       "  'm',\n",
       "  'n',\n",
       "  'o',\n",
       "  'on',\n",
       "  'one',\n",
       "  'r',\n",
       "  's',\n",
       "  't',\n",
       "  'u',\n",
       "  'v',\n",
       "  'y'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_pair_encoding([\"Movies\", \"are\", \"fun\", \"for\", \"everyone\", \"every\", \"time\", \"one\"], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1ce6211",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T05:07:04.501138Z",
     "start_time": "2025-04-16T05:06:43.703947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_for', 'u', 'm']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_word(word, merges, vocabulary, charset, unk_token=\"<UNK>\"):\n",
    "    word = '_' + word\n",
    "    if word in vocabulary:\n",
    "        return [word]\n",
    "    tokens = [char if char in charset else unk_token for char in word]\n",
    "    for left, right in merges:\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            if tokens[i:i + 2] == [left, right]:\n",
    "                tokens[i:i + 2] = [left + right]\n",
    "            else:\n",
    "                i += 1\n",
    "    return tokens\n",
    "\n",
    "vocabulary, merges, charset, tokens = byte_pair_encoding([\"Movies\", \"are\", \"fun\", \"for\", \"everyone\", \"every\", \"time\", \"one\"], 50)\n",
    "\n",
    "tokenize_word(\"forum\", merges, vocabulary, charset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d3ed2898af9a809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T05:01:33.285361Z",
     "start_time": "2025-04-16T05:01:33.282510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_for', 'u', 'm']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary, merges, charset, tokens = byte_pair_encoding([\"Movies\", \"are\", \"fun\", \"for\", \"everyone\", \"every\", \"time\", \"one\"], 50)\n",
    "\n",
    "tokenize_word(\"forum\", merges, vocabulary, charset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18b8a7cd5fb75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
