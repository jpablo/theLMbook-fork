{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aburkov/theLMbook/blob/main/news_decoder_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <div style=\"background-color: #f4f6f7; padding: 15px; width: 80%;\">\n",
    "        <table style=\"width: 100%\">\n",
    "            <tr>\n",
    "                <td style=\"vertical-align: middle;\">\n",
    "                    <span style=\"font-size: 14px;\">\n",
    "                        A notebook for <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">The Hundred-Page Language Models Book</a> by Andriy Burkov<br><br>\n",
    "                        Code repository: <a href=\"https://github.com/aburkov/theLMbook\" target=\"_blank\" rel=\"noopener\">https://github.com/aburkov/theLMbook</a>\n",
    "                    </span>\n",
    "                </td>\n",
    "                <td style=\"vertical-align: middle;\">\n",
    "                    <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">\n",
    "                        <img src=\"https://thelmbook.com/img/book.png\" width=\"80px\" alt=\"The Hundred-Page Language Models Book\">\n",
    "                    </a>\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "</div>"
   ],
   "metadata": {
    "id": "dQ-tX7JpAdMB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decoder-based language model\n",
    "\n",
    "## Utility functions and classes\n",
    "\n",
    "In the cell below, we import the dependencies and define the utility functions and the model class:"
   ],
   "metadata": {
    "id": "kb9Akwe7xttX"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yy0zjL_2ouOU",
    "ExecuteTime": {
     "end_time": "2025-06-09T06:38:23.032258Z",
     "start_time": "2025-06-09T06:38:19.364183Z"
    }
   },
   "source": [
    "# Import required libraries\n",
    "import os               # For file and path operations (check_file_exists, extract_dataset)\n",
    "import urllib.request   # For downloading dataset files from URLs\n",
    "import tarfile          # For extracting .tar.gz dataset archives\n",
    "import torch            # Main PyTorch library for tensor operations and deep learning\n",
    "import torch.nn as nn   # Neural network modules, layers, and utilities\n",
    "import torch.nn.functional as F  # For softmax\n",
    "from torch.utils.data import DataLoader, IterableDataset  # For efficient data loading\n",
    "import random           # For setting random seeds\n",
    "from tqdm import tqdm   # For progress bars\n",
    "import math             # For computing perplexity using exp()\n",
    "import re               # For preprocessing text (replacing numbers with placeholders)\n",
    "from transformers import AutoTokenizer  # For loading pre-trained tokenizer\n",
    "#import tempfile         # For temporary file handling during extraction\n",
    "#import shutil           # For file operations during extraction\n",
    "\n",
    "# ----------------------------\n",
    "# Utility Functions\n",
    "# ----------------------------\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets random seeds for reproducibility across different Python libraries.\n",
    "    This ensures that random operations give the same results across runs.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed value for random number generation\n",
    "    \"\"\"\n",
    "    # Set seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    # Set seed for PyTorch's CPU random number generator\n",
    "    torch.manual_seed(seed)\n",
    "    # Set seed for PyTorch's GPU random number generator\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # Requests cuDNN to use deterministic algorithms when possible\n",
    "    # Note: This may impact performance and might not guarantee determinism in all cases\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # Disables cuDNN's auto-tuner which finds the best algorithm for your specific input size\n",
    "    # Ensures consistent behavior but might be slower as it doesn't optimize for input sizes\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset Class\n",
    "# ----------------------------\n",
    "\n",
    "class IterableTextDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    An iterable dataset for processing text data in a memory-efficient way.\n",
    "    Instead of loading all data into memory, it streams data from disk.\n",
    "    Inherits from PyTorch's IterableDataset for streaming support.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file containing sentences\n",
    "        tokenizer: Tokenizer object for converting text to tokens\n",
    "        max_length (int): Maximum sequence length to process (default: 30)\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, tokenizer, max_length=30):\n",
    "        # Store file path for reading data\n",
    "        self.file_path = file_path\n",
    "        # Store tokenizer for text processing\n",
    "        self.tokenizer = tokenizer\n",
    "        # Set maximum sequence length to truncate long sequences\n",
    "        self.max_length = max_length\n",
    "        self._count_sentences()\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Creates an iterator over the dataset.\n",
    "        This method is called when iterating over the dataset.\n",
    "\n",
    "        Yields:\n",
    "            tuple: (input_sequence, target_sequence) pairs for language modeling\n",
    "                  input_sequence is the sequence up to the last token\n",
    "                  target_sequence is the sequence shifted one position right\n",
    "        \"\"\"\n",
    "        # Open file in read mode with UTF-8 encoding\n",
    "        with open(self.file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            # Process each line (sentence) in the file\n",
    "            for line in f:\n",
    "                # Remove leading/trailing whitespace\n",
    "                sentence = line.strip()\n",
    "                # Replace all numbers with ### placeholder\n",
    "                # This reduces vocabulary size and helps model generalize\n",
    "                sentence = re.sub(r\"\\d+\", \"###\", sentence)\n",
    "\n",
    "                # Convert sentence to token IDs\n",
    "                encoded_sentence = self.tokenizer.encode(\n",
    "                    sentence,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True\n",
    "                )\n",
    "\n",
    "                # Only use sequences with at least 2 tokens\n",
    "                # (need at least one input and one target token)\n",
    "                if len(encoded_sentence) >= 2:\n",
    "                    # Input is all tokens except last\n",
    "                    input_seq = encoded_sentence[:-1]\n",
    "                    # Target is all tokens except first\n",
    "                    target_seq = encoded_sentence[1:]\n",
    "                    # Convert to PyTorch tensors and yield\n",
    "                    yield torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return self._num_sentences\n",
    "\n",
    "    def _count_sentences(self):\n",
    "        print(f\"\\nCounting sentences in {self.file_path}...\")\n",
    "        with open(self.file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            self._num_sentences = sum(1 for _ in f)\n",
    "        print(f\"\\nFound {self._num_sentences} sentences in {self.file_path}.\")\n",
    "\n",
    "## ----------------------------\n",
    "## Download and prepare data\n",
    "## ----------------------------\n",
    "\n",
    "def create_collate_fn(tokenizer):\n",
    "    \"\"\"\n",
    "    Creates a collate function for batching sequences of different lengths.\n",
    "    This function pads shorter sequences to match the longest sequence in the batch.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Tokenizer object containing padding token information\n",
    "\n",
    "    Returns:\n",
    "        function: Collate function that handles padding in batches\n",
    "    \"\"\"\n",
    "    def collate_fn(batch):\n",
    "        # Separate inputs and targets from batch\n",
    "        input_seqs, target_seqs = zip(*batch)\n",
    "        # Get padding token ID from tokenizer\n",
    "        pad_index = tokenizer.pad_token_id\n",
    "        # Pad input sequences to same length\n",
    "        input_padded = nn.utils.rnn.pad_sequence(input_seqs, batch_first=True, padding_value=pad_index)\n",
    "        # Pad target sequences to same length\n",
    "        target_padded = nn.utils.rnn.pad_sequence(target_seqs, batch_first=True, padding_value=pad_index)\n",
    "        return input_padded, target_padded\n",
    "    return collate_fn\n",
    "\n",
    "def check_file_exists(filename):\n",
    "    \"\"\"\n",
    "    Checks if a file exists in the current directory.\n",
    "    Args:\n",
    "        filename (str): Name of the file to check\n",
    "    Returns:\n",
    "        bool: True if file exists, False otherwise\n",
    "    \"\"\"\n",
    "    return os.path.exists(filename)\n",
    "\n",
    "def download_file(url):\n",
    "    \"\"\"\n",
    "    Downloads a file from the given URL if it doesn't exist locally.\n",
    "    Uses a custom User-Agent to help prevent download blocks.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the file to download\n",
    "    Returns:\n",
    "        str: Name of the downloaded file (\"news.tar.gz\")\n",
    "    \"\"\"\n",
    "    # Always use news.tar.gz as the filename, regardless of URL\n",
    "    filename = \"news.tar.gz\"\n",
    "\n",
    "    if not check_file_exists(filename):\n",
    "        print(f\"\\nDownloading dataset from {url}...\")\n",
    "        req = urllib.request.Request(\n",
    "            url,\n",
    "            headers={\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        )\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            with open(filename, \"wb\") as out_file:\n",
    "                out_file.write(response.read())\n",
    "        print(\"\\nDownload completed.\")\n",
    "    else:\n",
    "        print(f\"\\n{filename} already downloaded.\")\n",
    "    return filename\n",
    "\n",
    "def is_within_directory(directory, target):\n",
    "    \"\"\"\n",
    "    Checks if a target path is within a specified directory by comparing absolute paths.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Base directory path\n",
    "        target (str): Target path to check\n",
    "    Returns:\n",
    "        bool: True if target's absolute path starts with directory's absolute path\n",
    "    \"\"\"\n",
    "    abs_directory = os.path.abspath(directory)\n",
    "    abs_target = os.path.abspath(target)\n",
    "    prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "    return prefix == abs_directory\n",
    "\n",
    "def extract_dataset(filename):\n",
    "    \"\"\"\n",
    "    Extracts train.txt and test.txt from the downloaded archive.\n",
    "    Includes debug information about archive contents.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Name of the archive file\n",
    "    Returns:\n",
    "        tuple: Paths to extracted train and test files\n",
    "    \"\"\"\n",
    "    data_dir = os.path.join(os.path.dirname(filename), \"news\")\n",
    "    train_path = os.path.join(data_dir, \"train.txt\")\n",
    "    test_path = os.path.join(data_dir, \"test.txt\")\n",
    "\n",
    "    if check_file_exists(train_path) and check_file_exists(test_path):\n",
    "        print(\"\\nData files already extracted.\")\n",
    "        return train_path, test_path\n",
    "\n",
    "    print(\"\\nListing archive contents:\")\n",
    "    with tarfile.open(filename, \"r:gz\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            print(f\"\\nArchive member: {member.name}\")\n",
    "\n",
    "        print(\"\\nExtracting files...\")\n",
    "        # Extract to current directory first\n",
    "        tar.extractall('.')\n",
    "\n",
    "    if not (check_file_exists(train_path) and check_file_exists(test_path)):\n",
    "        raise FileNotFoundError(f\"\\nRequired files not found in the archive. Please check the paths above.\")\n",
    "\n",
    "    print(\"\\nExtraction completed.\")\n",
    "    return train_path, test_path\n",
    "\n",
    "def create_datasets(train_file, test_file, tokenizer, max_length=30):\n",
    "    \"\"\"\n",
    "    Creates IterableTextDataset objects for training and testing.\n",
    "    These datasets will stream data from disk instead of loading it all into memory.\n",
    "\n",
    "    Args:\n",
    "        train_file (str): Path to training data file\n",
    "        test_file (str): Path to test data file\n",
    "        tokenizer: Tokenizer object for text processing\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_dataset, test_dataset) - Dataset objects for training and testing\n",
    "    \"\"\"\n",
    "    # Create training dataset\n",
    "    train_dataset = IterableTextDataset(train_file, tokenizer, max_length)\n",
    "    # Create test dataset\n",
    "    test_dataset = IterableTextDataset(test_file, tokenizer, max_length)\n",
    "\n",
    "    # Print dataset sizes\n",
    "    print(f\"\\nTraining sentences: {len(train_dataset)}\")\n",
    "    print(f\"\\nTest sentences: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def create_dataloaders(train_dataset, test_dataset, batch_size, collate_fn):\n",
    "    \"\"\"\n",
    "    Creates DataLoader objects for efficient data iteration.\n",
    "\n",
    "    Args:\n",
    "        train_dataset: Training dataset\n",
    "        test_dataset: Test dataset\n",
    "        batch_size (int): Number of sequences per batch\n",
    "        collate_fn: Function to handle padding and batch creation\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_dataloader, test_dataloader) - DataLoader objects for\n",
    "               iterating over batches of data with proper padding\n",
    "    \"\"\"\n",
    "    # Create training data loader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,    # Function to handle padding\n",
    "        num_workers=0             # Number of worker processes (0 = single process)\n",
    "    )\n",
    "    # Create test data loader\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "def download_and_prepare_data(url, batch_size, tokenizer, max_length=30):\n",
    "    \"\"\"\n",
    "    Main function to handle the complete data preparation pipeline.\n",
    "    Downloads data, extracts it, and creates necessary dataset objects.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL where the dataset archive can be downloaded\n",
    "        batch_size (int): Batch size for data loading\n",
    "        tokenizer: Tokenizer object for text processing\n",
    "        max_length (int): Maximum sequence length for tokenization (default: 30)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_dataloader, test_dataloader) - Ready-to-use data loaders\n",
    "    \"\"\"\n",
    "    # Step 1: Download dataset archive from URL\n",
    "    filename = download_file(url)\n",
    "\n",
    "    # Step 2: Extract training and test files from archive\n",
    "    train_file, test_file = extract_dataset(filename)\n",
    "\n",
    "    # Step 3: Create dataset objects for streaming data\n",
    "    train_dataset, test_dataset = create_datasets(train_file, test_file, tokenizer, max_length)\n",
    "\n",
    "    # Step 4: Create function to handle batch creation\n",
    "    collate_fn = create_collate_fn(tokenizer)\n",
    "\n",
    "    # Step 5: Create and return data loaders\n",
    "    return create_dataloaders(train_dataset, test_dataset, batch_size, collate_fn)\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation Functions\n",
    "# ----------------------------\n",
    "\n",
    "def compute_loss_and_perplexity(model, dataloader, tokenizer, criterion, device, max_sentences=1000):\n",
    "    \"\"\"\n",
    "    Evaluates model performance by computing loss and perplexity on data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The language model to evaluate\n",
    "        dataloader (DataLoader): Data loader containing batched sequences\n",
    "        tokenizer: Tokenizer for handling special tokens like padding\n",
    "        criterion: Loss function (usually CrossEntropyLoss)\n",
    "        device: Device to run computation on (cuda/cpu)\n",
    "        max_sentences (int): Maximum number of sentences to evaluate (default: 1000)\n",
    "                           Limits evaluation to a subset for faster validation\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, perplexity, sentences_processed)\n",
    "               - average_loss: Mean loss per token (excluding padding)\n",
    "               - perplexity: exp(average_loss), lower is better\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode (disables dropout, etc.)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters for loss calculation\n",
    "    total_loss = 0.0          # Accumulator for total loss across all batches\n",
    "    total_tokens = 0          # Counter for total number of tokens (excluding padding)\n",
    "    sentences_processed = 0    # Counter for number of sentences processed\n",
    "\n",
    "    # Disable gradient computation for efficiency\n",
    "    with torch.no_grad():\n",
    "        # Iterate through data with progress bar\n",
    "        for input_seq, target_seq in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            # Move input and target sequences to specified device\n",
    "            input_seq = input_seq.to(device)      # Shape: (batch_size, seq_len)\n",
    "            target_seq = target_seq.to(device)    # Shape: (batch_size, seq_len)\n",
    "\n",
    "            # Get current batch size (might be smaller for last batch)\n",
    "            batch_size_current = input_seq.size(0)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            logits = model(input_seq)             # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "            # Reshape logits and target for loss calculation\n",
    "            logits = logits.reshape(-1, logits.size(-1))  # Shape: (batch_size * seq_len, vocab_size)\n",
    "            target = target_seq.reshape(-1)              # Shape: (batch_size * seq_len)\n",
    "\n",
    "            # Create mask to exclude padding tokens\n",
    "            mask = target != tokenizer.pad_token_id\n",
    "\n",
    "            # Compute loss only on non-padded tokens\n",
    "            loss = criterion(logits[mask], target[mask])\n",
    "\n",
    "            # Update counters\n",
    "            loss_value = loss.item() * mask.sum().item()  # Total loss for this batch\n",
    "            total_loss += loss_value                      # Accumulate batch loss\n",
    "            total_tokens += mask.sum().item()             # Count non-padding tokens\n",
    "\n",
    "            # Update sentence counter and check if we've reached maximum\n",
    "            sentences_processed += batch_size_current\n",
    "            if sentences_processed >= max_sentences:\n",
    "                break\n",
    "\n",
    "    # Calculate final metrics\n",
    "    average_loss = total_loss / total_tokens           # Normalize loss by number of tokens\n",
    "    perplexity = math.exp(average_loss)               # Convert loss to perplexity\n",
    "\n",
    "    return average_loss, perplexity\n",
    "\n",
    "def generate_text(model, start_string, tokenizer, device, max_length=50):\n",
    "    \"\"\"\n",
    "    Generates text continuation from a given start string using greedy decoding.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained language model\n",
    "        start_string (str): Initial text to continue from\n",
    "        tokenizer: Tokenizer for text processing\n",
    "        device: Device to run generation on (cuda/cpu)\n",
    "        max_length (int): Maximum length of generated sequence\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text continuation\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode to disable dropout and other training-specific behaviors\n",
    "    model.eval()\n",
    "\n",
    "    # Convert input string to token indices\n",
    "    input_indices = tokenizer.encode(start_string, add_special_tokens=False)\n",
    "\n",
    "    # Convert indices to tensor and move to specified device (GPU/CPU)\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long).to(device)\n",
    "\n",
    "    # Keep track of all generated tokens, starting with input sequence\n",
    "    generated_indices = input_indices.copy()\n",
    "\n",
    "    # Generate tokens until we hit max length or end-of-sequence token\n",
    "    for _ in range(max_length - len(input_indices)):\n",
    "        # Get model predictions for the entire sequence\n",
    "        logits = model(input_tensor)\n",
    "        # Only take predictions for the last token position\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Prevent the model from generating unknown tokens by setting their probability to negative infinity\n",
    "        if tokenizer.unk_token_id is not None:\n",
    "            logits[:, tokenizer.unk_token_id] = float(\"-inf\")\n",
    "\n",
    "        # Greedy decoding: select the token with highest probability\n",
    "        next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Add the chosen token to our generated sequence\n",
    "        generated_indices.append(next_token.item())\n",
    "\n",
    "        # If we generate an end-of-sequence token, stop generation\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Add the new token to input tensor for next iteration\n",
    "        input_tensor = torch.cat([input_tensor, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "    # Convert token indices back to text, removing any special tokens\n",
    "    return tokenizer.decode(generated_indices, skip_special_tokens=True)\n",
    "\n",
    "def save_model(model, tokenizer, model_name):\n",
    "    \"\"\"\n",
    "    Saves the model state dictionary and tokenizer using the specified model name.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model to save\n",
    "        tokenizer: The tokenizer used with the model\n",
    "        model_name (str): Name to use for the saved model files\n",
    "    \"\"\"\n",
    "    # Create the models directory if it doesn't exist\n",
    "    save_dir = os.path.join(\"models\", model_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the model state dictionary and configuration\n",
    "    model_path = os.path.join(save_dir, f\"{model_name}.pth\")\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"model_config\": {\n",
    "            \"vocab_size\": len(tokenizer),\n",
    "            \"emb_dim\": model.embedding.embedding_dim,\n",
    "            \"num_heads\": len(model.layers[0].attn.heads),\n",
    "            \"num_blocks\": len(model.layers),\n",
    "            \"pad_idx\": model.embedding.padding_idx\n",
    "        }\n",
    "    }, model_path)\n",
    "\n",
    "    # Save the tokenizer\n",
    "    tokenizer_path = os.path.join(save_dir, \"tokenizer\")\n",
    "    tokenizer.save_pretrained(tokenizer_path)\n",
    "\n",
    "    print(f\"Model and tokenizer saved as '{model_name}'\")\n",
    "\n",
    "def load_model(model_name, device=None):\n",
    "    \"\"\"\n",
    "    Loads a saved model and tokenizer using the model name.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model to load\n",
    "        device: Device to load the model onto (if None, uses available device)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (loaded_model, loaded_tokenizer)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    save_dir = os.path.join(\"models\", model_name)\n",
    "\n",
    "    # Check if model exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        raise FileNotFoundError(f\"No saved model found with name '{model_name}'\")\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer_path = os.path.join(save_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "    # Load the model state and config\n",
    "    model_path = os.path.join(save_dir, f\"{model_name}.pth\")\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "    # Create a new model instance with the saved configuration\n",
    "    model = DecoderLanguageModel(\n",
    "        vocab_size=checkpoint[\"model_config\"][\"vocab_size\"],\n",
    "        emb_dim=checkpoint[\"model_config\"][\"emb_dim\"],\n",
    "        num_heads=checkpoint[\"model_config\"][\"num_heads\"],\n",
    "        num_blocks=checkpoint[\"model_config\"][\"num_blocks\"],\n",
    "        pad_idx=checkpoint[\"model_config\"][\"pad_idx\"]\n",
    "    )\n",
    "\n",
    "    # Load the saved state dictionary\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"\\nModel '{model_name}' loaded successfully\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_hyperparameters():\n",
    "    emb_dim = 128\n",
    "    num_heads = 8\n",
    "    num_blocks = 2\n",
    "    batch_size = 128\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 1\n",
    "    context_size = 30\n",
    "    return emb_dim, num_heads, num_blocks, batch_size, learning_rate, num_epochs, context_size"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Classes\n",
    "\n",
    "Decoder Transformer language model classes and the initialization method:"
   ],
   "metadata": {
    "id": "7dSK1bTyku7F"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ----------------------------\n",
    "# Weight Initialization and Core Functions\n",
    "# This section contains utility functions for weight initialization\n",
    "# and core computational functions used throughout the model\n",
    "# ----------------------------\n",
    "\n",
    "def initialize_weights(model):\n",
    "    \"\"\"\n",
    "    Initialize the weights of different model components using appropriate schemes.\n",
    "    Each layer type receives specialized initialization for optimal training.\n",
    "    \"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Xavier uniform initialization for linear layers\n",
    "            # Helps maintain variance across network layers\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)  # Initialize biases to zero\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # Initialize embedding layers with normal distribution\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                # Ensure padding tokens have zero embeddings\n",
    "                with torch.no_grad():\n",
    "                    module.weight[module.padding_idx].fill_(0)\n",
    "        elif isinstance(module, AttentionHead):\n",
    "            # Initialize query, key, and value projection matrices\n",
    "            # Xavier uniform helps maintain good gradient flow\n",
    "            nn.init.xavier_uniform_(module.W_Q)\n",
    "            nn.init.xavier_uniform_(module.W_K)\n",
    "            nn.init.xavier_uniform_(module.W_V)\n",
    "        elif isinstance(module, MultiHeadAttention):\n",
    "            # Initialize output projection matrix for attention mechanism\n",
    "            nn.init.xavier_uniform_(module.W_O)\n",
    "        elif isinstance(module, DecoderLanguageModel):\n",
    "            # Initialize final output projection layer\n",
    "            nn.init.xavier_uniform_(module.output)\n",
    "        elif isinstance(module, RMSNorm):\n",
    "            # Initialize RMSNorm scale parameters to ones\n",
    "            # This starts with identity transformation\n",
    "            nn.init.ones_(module.scale)\n",
    "        elif isinstance(module, MLP):\n",
    "            # Initialize feed-forward network parameters\n",
    "            nn.init.xavier_uniform_(module.W_1)\n",
    "            nn.init.xavier_uniform_(module.W_2)\n",
    "            nn.init.zeros_(module.B_1)\n",
    "            nn.init.zeros_(module.B_2)\n",
    "\n",
    "def rope(x, theta_base=10000.0):\n",
    "    \"\"\"\n",
    "    Implements Rotary Position Embedding (RoPE) for transformer attention.\n",
    "    RoPE encodes position information through rotation matrices applied to pairs of dimensions.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (batch_size, seq_len, emb_dim)\n",
    "        theta_base: Base for computing rotation frequencies (default: 10000.0)\n",
    "\n",
    "    Returns:\n",
    "        Tensor with position information encoded through rotations\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, emb_dim = x.size()\n",
    "    assert emb_dim % 2 == 0, \"Embedding dimensionality must be even for RoPE\"\n",
    "\n",
    "    # Generate sequence position indices\n",
    "    pos = torch.arange(0, seq_len, dtype=torch.float32, device=x.device)\n",
    "    pos = pos.unsqueeze(0).expand(batch_size, seq_len)\n",
    "\n",
    "    # Compute frequency bands for each dimension pair\n",
    "    # Modified: frequencies start from p=1 and use (p-1) in exponent\n",
    "    p = torch.arange(1, emb_dim // 2 + 1, dtype=torch.float32, device=x.device)\n",
    "    theta_p = 1.0 / (theta_base ** (2 * (p - 1) / emb_dim))\n",
    "\n",
    "    # Compute rotation angles for each position and frequency\n",
    "    pos = pos.unsqueeze(-1)\n",
    "    theta = pos * theta_p\n",
    "\n",
    "    # Compute rotation components\n",
    "    sin_theta = torch.sin(theta)\n",
    "    cos_theta = torch.cos(theta)\n",
    "\n",
    "    # Split input into alternating dimensions\n",
    "    x1 = x[..., 0::2]  # Dimensions at indices 0,2,4,...\n",
    "    x2 = x[..., 1::2]  # Dimensions at indices 1,3,5,...\n",
    "\n",
    "    # Apply 2D rotations to each pair\n",
    "    x_rotated_1 = x1 * cos_theta - x2 * sin_theta\n",
    "    x_rotated_2 = x1 * sin_theta + x2 * cos_theta\n",
    "\n",
    "    # Recombine rotated pairs into final output\n",
    "    x_rotated = torch.stack((x_rotated_1, x_rotated_2), dim=-1).reshape(batch_size, seq_len, emb_dim)\n",
    "\n",
    "    return x_rotated\n",
    "\n",
    "# ----------------------------\n",
    "# Model Components\n",
    "# This section contains the building blocks of the transformer decoder\n",
    "# including normalization, attention, and feed-forward layers\n",
    "# ----------------------------\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization\n",
    "    A simplified alternative to Layer Normalization that only uses RMS statistics\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))  # Learnable scale parameter\n",
    "        self.epsilon = epsilon  # Small constant for numerical stability\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute root mean square normalization\n",
    "        squared_x = x ** 2\n",
    "        mean_squared = torch.mean(squared_x, dim=-1, keepdim=True)\n",
    "        rms = torch.sqrt(mean_squared + self.epsilon)\n",
    "\n",
    "        # Normalize and scale\n",
    "        x_normalized = x / rms\n",
    "        output = x_normalized * self.scale\n",
    "        return output\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Single head of self-attention\n",
    "    Transforms input using learned projections and computes scaled dot-product attention\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, d_h):\n",
    "        super().__init__()\n",
    "        # Initialize projection matrices for queries, keys, and values\n",
    "        self.W_Q = nn.Parameter(torch.rand(emb_dim, d_h))\n",
    "        self.W_K = nn.Parameter(torch.rand(emb_dim, d_h))\n",
    "        self.W_V = nn.Parameter(torch.rand(emb_dim, d_h))\n",
    "        self.d_h = d_h  # Dimensionality of attention head\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Project input into query, key, and value spaces\n",
    "        Q = x @ self.W_Q\n",
    "        K = x @ self.W_K\n",
    "        V = x @ self.W_V\n",
    "\n",
    "        # Apply rotary position embeddings to queries and keys\n",
    "        Q, K = rope(Q), rope(K)\n",
    "\n",
    "        # Compute attention scores with scaling factor\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_h)\n",
    "\n",
    "        # Apply causal mask and attention weights\n",
    "        masked_scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attention_weights = torch.softmax(masked_scores, dim=-1)\n",
    "\n",
    "        return attention_weights @ V\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism\n",
    "    Allows the model to jointly attend to information from different positions\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        d_h = emb_dim // num_heads  # Dimensionality of each attention head\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(emb_dim, d_h)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        # Output projection matrix\n",
    "        self.W_O = nn.Parameter(torch.rand(emb_dim, emb_dim))\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Process input through each attention head\n",
    "        head_outputs = [head(x, mask) for head in self.heads]\n",
    "\n",
    "        # Concatenate outputs and project to final dimensionality\n",
    "        x = torch.cat(head_outputs, dim=-1)\n",
    "        return x @ self.W_O\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for transformer feed-forward network\n",
    "    Uses a larger intermediate dimensionality (4x) with ReLU activation\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        # Initialize weights and biases for two-layer feed-forward network\n",
    "        self.W_1 = nn.Parameter(torch.rand(emb_dim, emb_dim * 4))\n",
    "        self.B_1 = nn.Parameter(torch.rand(emb_dim * 4))\n",
    "        self.W_2 = nn.Parameter(torch.rand(emb_dim * 4, emb_dim))\n",
    "        self.B_2 = nn.Parameter(torch.rand(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First linear transformation and activation\n",
    "        x = x @ self.W_1 + self.B_1\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Second linear transformation\n",
    "        x = x @ self.W_2 + self.B_2\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single transformer decoder block\n",
    "    Combines self-attention and feed-forward layers with residual connections\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        # Layer components\n",
    "        self.norm1 = RMSNorm(emb_dim)\n",
    "        self.attn = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.norm2 = RMSNorm(emb_dim)\n",
    "        self.mlp = MLP(emb_dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Self-attention sub-block with residual connection\n",
    "        attn_out = self.attn(self.norm1(x), mask)\n",
    "        x = x + attn_out\n",
    "\n",
    "        # Feed-forward sub-block with residual connection\n",
    "        mlp_out = self.mlp(self.norm2(x))\n",
    "        x = x + mlp_out\n",
    "        return x\n",
    "\n",
    "class DecoderLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete decoder-only transformer language model\n",
    "    Processes input sequences using multiple decoder blocks and projects to vocabulary\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim, num_heads, num_blocks, pad_idx):\n",
    "        super().__init__()\n",
    "        # Token embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Stack of decoder blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(emb_dim, num_heads) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        # Output projection to vocabulary size\n",
    "        self.output = nn.Parameter(torch.rand(emb_dim, vocab_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed input tokens\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Create causal attention mask\n",
    "        _, seq_len, _ = x.size()\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device))\n",
    "\n",
    "        # Process through decoder blocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        # Project to vocabulary distribution\n",
    "        return x @ self.output"
   ],
   "metadata": {
    "id": "Iz7RlKWvk0Wk",
    "ExecuteTime": {
     "end_time": "2025-06-09T07:06:44.012599Z",
     "start_time": "2025-06-09T07:06:43.998827Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the language model\n",
    "\n",
    "In the cell below, we load the data, train, and save the language model:"
   ],
   "metadata": {
    "id": "RqQxwedtxESQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ----------------------------\n",
    "# Main training loop for a Decoder Language Model\n",
    "# This script handles the entire training process including data loading,\n",
    "# model training, validation, and text generation\n",
    "# ----------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize random seeds to ensure reproducible results\n",
    "    set_seed(42)\n",
    "\n",
    "    # Retrieve model architecture and training hyperparameters from configuration\n",
    "    # emb_dim: dimensionality of input token and intermediary embeddings\n",
    "    # num_heads: number of attention heads in each transformer block\n",
    "    # num_blocks: number of transformer blocks in the model\n",
    "    # batch_size: mini-batch size\n",
    "    # learning_rate: step size for optimizer updates\n",
    "    # num_epochs: number of complete passes through the training dataset\n",
    "    # context_size: maximum input sequence length\n",
    "    emb_dim, num_heads, num_blocks, batch_size, learning_rate, num_epochs, context_size = get_hyperparameters()\n",
    "\n",
    "    # Initialize the tokenizer using Microsoft's Phi-3.5-mini model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "    # Get padding token index for padding shorter sequences\n",
    "    pad_idx = tokenizer.pad_token_id\n",
    "\n",
    "    # Check for CUDA-capable GPU and set the device accordingly\n",
    "    device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "    # Download the news dataset and create DataLoader objects for training and testing\n",
    "    # DataLoaders handle batching and shuffling\n",
    "    data_url = \"https://www.thelmbook.com/data/news\"\n",
    "    train_dataloader, test_dataloader = download_and_prepare_data(\n",
    "        data_url, batch_size, tokenizer, context_size\n",
    "    )\n",
    "\n",
    "    # Get the size of the vocabulary that the model needs to handle\n",
    "    vocab_size = len(tokenizer)\n",
    "    print(f\"\\nVocabulary size: {vocab_size}\\n\")\n",
    "\n",
    "    # Initialize the Decoder language model with specified architecture parameters\n",
    "    # vocab_size: determines output layer dimensionality\n",
    "    # emb_dim: size of token embeddings and intermediary embeddings\n",
    "    # num_heads: number of attention heads per transformer block\n",
    "    # num_blocks: number of transformer blocks in the model\n",
    "    # pad_idx: special token ID used for padding shorter sequences\n",
    "    model = DecoderLanguageModel(\n",
    "        vocab_size, emb_dim, num_heads, num_blocks, pad_idx\n",
    "    )\n",
    "\n",
    "    # Move the model to GPU if available\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize model weights using custom initialization scheme\n",
    "    # This is important for stable training of deep neural networks\n",
    "    initialize_weights(model)\n",
    "\n",
    "    # Initialize the AdamW optimizer with specified learning rate\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize the loss function (Cross Entropy) for training\n",
    "    # ignore_index=pad_idx ensures that padding tokens don't contribute to the loss\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "    # Calculate and display the total number of trainable parameters in the model\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal trainable parameters: {total_params}\\n\")\n",
    "\n",
    "    # Set evaluation interval (number of examples after which to perform validation)\n",
    "    # 200,000 examples provides a good balance between training time and monitoring frequency\n",
    "    eval_interval = 200_000\n",
    "    examples_processed = 0  # Counter for tracking progress toward next evaluation\n",
    "\n",
    "    # Define test contexts for generating sample text during evaluation\n",
    "    contexts = [\n",
    "        \"Moscow\",\n",
    "        \"New York\",\n",
    "        \"A hurricane\",\n",
    "        \"The President\"\n",
    "    ]\n",
    "\n",
    "    # Main training loop - iterate through specified number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Initialize tracking variables for this epoch\n",
    "        total_loss = 0.0      # Accumulator for loss across all batches\n",
    "        total_tokens = 0      # Counter for actual tokens processed (excluding padding)\n",
    "\n",
    "        # Create progress bar for monitoring training progress\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Iterate through batches in the training data\n",
    "        for batch_idx, (input_seq, target_seq) in enumerate(progress_bar):\n",
    "            # Move input and target sequences to GPU if available\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            # Clear gradients from previous batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: get model predictions for this batch\n",
    "            # output shape: (batch_size, seq_len, vocab_size)\n",
    "            logits = model(input_seq)\n",
    "\n",
    "            # Reshape logits and target tensors for loss computation\n",
    "            logits = logits.reshape(-1, logits.size(-1))\n",
    "            target = target_seq.reshape(-1)\n",
    "\n",
    "            # Create mask to exclude padding tokens from loss calculation\n",
    "            mask = target != pad_idx\n",
    "\n",
    "            # Compute loss between model predictions and actual targets\n",
    "            # Using masked versions to ignore padding tokens\n",
    "            loss = criterion(logits[mask], target[mask])\n",
    "\n",
    "            # Backward pass: compute gradients of loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Update model parameters using calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate actual loss value for this batch accounting for padding\n",
    "            loss_value = loss.item() * mask.sum().item()\n",
    "\n",
    "            # Accumulate total loss and tokens for epoch statistics\n",
    "            total_loss += loss_value\n",
    "            total_tokens += mask.sum().item()\n",
    "            examples_processed += input_seq.size(0)\n",
    "\n",
    "            # Update progress bar with current batch loss\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "            # Periodic evaluation after processing specified number of examples\n",
    "            if examples_processed >= eval_interval:\n",
    "                # Calculate average loss over the last eval_interval examples\n",
    "                avg_loss = total_loss / total_tokens\n",
    "                print(f\"\\nAfter {examples_processed} examples, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "                # Switch to evaluation mode\n",
    "                model.eval()\n",
    "\n",
    "                # Compute validation metrics\n",
    "                average_loss, perplexity = compute_loss_and_perplexity(\n",
    "                    model, test_dataloader, tokenizer, criterion, device, max_sentences=1000\n",
    "                )\n",
    "                # Record validation\n",
    "                print(f\"\\nValidation Average Loss: {average_loss:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "                model.eval()\n",
    "\n",
    "                # Generate sample texts to qualitatively assess model performance\n",
    "                for context in contexts:\n",
    "                    # Generate text continuation for each test context\n",
    "                    generated_text = generate_text(\n",
    "                        model=model,\n",
    "                        start_string=context,\n",
    "                        tokenizer=tokenizer,\n",
    "                        device=device,\n",
    "                        max_length=50\n",
    "                    )\n",
    "                    print(f\"\\nContext: {context}\")\n",
    "                    print(f\"\\nGenerated text: {generated_text}\\n\")\n",
    "\n",
    "                # Switch back to training mode for continued training\n",
    "                model.train()\n",
    "\n",
    "                # Reset counters for next evaluation interval\n",
    "                examples_processed = 0\n",
    "                total_loss = 0.0\n",
    "                total_tokens = 0\n",
    "\n",
    "        # End-of-epoch reporting\n",
    "        if total_tokens > 0:\n",
    "            # Calculate and display average loss for the epoch\n",
    "            avg_loss = total_loss / total_tokens\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        else:\n",
    "            # Handle edge case where no tokens were processed\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs} completed.\")\n",
    "\n",
    "        # Perform end-of-epoch validation\n",
    "        model.eval()\n",
    "\n",
    "        # Generate sample texts for qualitative assessment\n",
    "        print(\"\\nGenerating text based on contexts using generate_text:\\n\")\n",
    "        for context in contexts:\n",
    "            generated_text = generate_text(\n",
    "                model=model,\n",
    "                start_string=context,\n",
    "                tokenizer=tokenizer,\n",
    "                device=device,\n",
    "                max_length=50\n",
    "            )\n",
    "            print(f\"\\nContext: {context}\")\n",
    "            print(f\"\\nGenerated text: {generated_text}\\n\")\n",
    "\n",
    "        average_loss, perplexity = compute_loss_and_perplexity(\n",
    "            model, test_dataloader, tokenizer, criterion, device, max_sentences=1000\n",
    "        )\n",
    "        print(f\"\\nValidation Average Loss: {average_loss:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "        # Reset to training mode for next epoch\n",
    "        model.train()\n",
    "\n",
    "    # Save the trained model and tokenizer for later use\n",
    "    # This includes model architecture, weights, and tokenizer configuration\n",
    "    model_name = \"Decoder_LM\"\n",
    "    save_model(model, tokenizer, model_name)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9a8699a145a94ed89b8b11d715067c3f",
      "b576e000e0d549b089431943ad050eba",
      "181b36d4f4e14c10adc7b249717e21e4",
      "2226577264e14049b08244ce001e4e31",
      "d7f2588a852742afa54c616961176830",
      "c36c028aeee14b95901e1fb005c7201a",
      "cd63f0fa54284235bb73c59bf5cb2a92",
      "ae3f1435c9714894810273ab6cc8fb39",
      "188a4756346f45f796f49a44e694ad29",
      "8873103295c3485ca41a3347f8766a9d",
      "2f54ab7c292246db81dc7cbab21c7cbb",
      "e41117d4fed1445daf6fb85a01cbc81e",
      "fefd7c010fdf4735aa9b2513db0497d0",
      "f48e51c77737475fb4aaafce03380fb7",
      "82778a1b41a0417b86fc1e29455fb383",
      "5b6201ac09a64fa79f255ac5a14b3a5a",
      "f46a10202f08457393536590b5084b3e",
      "32dc1d47b7a545ff9759d3897eee59d2",
      "12db66482d064d7e831767ccbcef1bec",
      "ee332c51eee94dcbb5586f463e39afdf",
      "ec1dd8303a3b4f5e97b3d218f3de8613",
      "09222bf385da4ce0be09da2e03b74f74",
      "6ba6646a5d5347059b14acdd79d6da07",
      "756c21cc62c14083a6a6729f54232d79",
      "0b2e8ac291734ae39059110f96493a18",
      "cf27f80668ff40c9ac07a93197b34640",
      "c65097f7cf4a4722a186cfb1822f0315",
      "beb1e66b6cc44e7490fb207bff6fbd2c",
      "3daf9a7b54124787b732fdb108089e00",
      "2adb1d17c4404860b49575ea15dbff13",
      "08718b87ba204a34a9449fc9a60f97ad",
      "72a5c19cce364fed959f66f6521a3cf0",
      "57c3488609f046c8852641b73b83e678",
      "00647dd5e8034abcb0217cc30e02177f",
      "57ef6e22ee014ce0b467d6ae328d37ec",
      "26c3ff709f804abcacac8861c235b6f4",
      "ae4d86bfa9914f6e81886ab56a011ec9",
      "677fb8e2b421433fa412f9d75e319081",
      "156cc956117a4ed984446f3d1a3aebe3",
      "260bd2215a284268b2920be0c458393c",
      "fadf515115e64ebeb234df1c3c9dce71",
      "20370596c8824147a0aab279ad6241d8",
      "bf8a3680974742c4962608eaac628a6e",
      "b9d3e3c74d644a27a6bfb7b374683f02",
      "6466f91527ee437ebd69ce5cb90bdf6f",
      "ecf37b3cecb8405685d622d1f3e05bcf",
      "3277a976d1034d12b12ec767ae80e3c5",
      "6a1bda5b60a1449caf1a7b1e9ef6e844",
      "1fa0d48eb7624fc8806942941ca4b411",
      "f17c35f573354ce78baf719054af4732",
      "6415e99c230c463298a92d761875fd2e",
      "0f34f0115c644766a1c6cc5b61485608",
      "6044a91e47104cb9b18a8add2a0e4c6f",
      "d808e2efb7b54ae79e431f56ab9fc871",
      "6aa8bfe1c8864a679e8634806ec46dac"
     ]
    },
    "id": "QCs-n1WaxFf9",
    "outputId": "7a39a591-0f98-4753-c2f9-cbb717a518f6",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-09T07:12:40.606081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading dataset from https://www.thelmbook.com/data/news...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s61ovCwawq3f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the model\n",
    "\n",
    "In the cell below, we load and test the language model:"
   ],
   "metadata": {
    "id": "Bze_jce90ZJg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezbawsEzZQVj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a9efdcc4-8f81-479e-e208-5089bfe092c6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Model 'Decoder_LM' loaded successfully\n",
      "\n",
      "Testing the model:\n",
      "\n",
      "\n",
      "Prompt: Moscow\n",
      "\n",
      "Generated response: Moscow , which has been the first of its ##,###-strong .###-###-###-### .### .### . 's . 's the . 'ed . ' '' . 's the\n",
      "\n",
      "\n",
      "Prompt: New York\n",
      "\n",
      "Generated response: New York City Mayor Michael Bloomberg said the company 's decision to take a breakdown of the new deal . '' 's . ' '' ' I . 's . 's . ' '' . 's . ' . ' . '\n",
      "\n",
      "\n",
      "Prompt: A hurricane\n",
      "\n",
      "Generated response: A hurricane season is expected to be completed in #### . '' ' I 'm not going to be a major factor . ' '' ' I 'm not going to be a matter of . ' '' ' . ' '' ' . ' ''\n",
      "\n",
      "\n",
      "Prompt: The President\n",
      "\n",
      "Generated response: The President of the House of Representatives has said that the bill would be a `` very important step '' . '' ' '' . ' '' ' I 'm not sure that the president of the House of Representatives of the House of Representatives . '' `` the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Model tests\n",
    "# ----------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model_name = \"Decoder_LM\"\n",
    "\n",
    "    # Load the previously saved model and tokenizer from disk\n",
    "    # This recreates the exact model state from after training\n",
    "    model, tokenizer = load_model(model_name)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Print header for test section\n",
    "    print(\"\\nTesting the model:\\n\")\n",
    "\n",
    "    # Define a list of test prompts to evaluate model performance\n",
    "    contexts = [\n",
    "        \"Moscow\",\n",
    "        \"New York\",\n",
    "        \"A hurricane\",\n",
    "        \"The President\"\n",
    "    ]\n",
    "\n",
    "    # Iterate through each test prompt and generate text\n",
    "    for context in contexts:\n",
    "        # Generate text using greedy decoding (most likely tokens)\n",
    "        generated_text = generate_text(\n",
    "            model=model,          # The loaded language model\n",
    "            start_string=context, # Text to continue\n",
    "            tokenizer=tokenizer,  # Tokenizer for text conversion\n",
    "            device=device,        # CPU or GPU device\n",
    "            max_length=50         # Maximum length of generated sequence\n",
    "        )\n",
    "        # Print the original prompt and model's response\n",
    "        print(f\"\\nPrompt: {context}\")\n",
    "        print(f\"\\nGenerated response: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ],
   "metadata": {
    "id": "x-ZB0pUgEZXA"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "authorship_tag": "ABX9TyP8DX5jT6kcvT1Sm5bOodq4",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "9a8699a145a94ed89b8b11d715067c3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b576e000e0d549b089431943ad050eba",
       "IPY_MODEL_181b36d4f4e14c10adc7b249717e21e4",
       "IPY_MODEL_2226577264e14049b08244ce001e4e31"
      ],
      "layout": "IPY_MODEL_d7f2588a852742afa54c616961176830"
     }
    },
    "b576e000e0d549b089431943ad050eba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c36c028aeee14b95901e1fb005c7201a",
      "placeholder": "",
      "style": "IPY_MODEL_cd63f0fa54284235bb73c59bf5cb2a92",
      "value": "tokenizer_config.json:100%"
     }
    },
    "181b36d4f4e14c10adc7b249717e21e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae3f1435c9714894810273ab6cc8fb39",
      "max": 3984,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_188a4756346f45f796f49a44e694ad29",
      "value": 3984
     }
    },
    "2226577264e14049b08244ce001e4e31": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8873103295c3485ca41a3347f8766a9d",
      "placeholder": "",
      "style": "IPY_MODEL_2f54ab7c292246db81dc7cbab21c7cbb",
      "value": "3.98k/3.98k[00:00&lt;00:00,344kB/s]"
     }
    },
    "d7f2588a852742afa54c616961176830": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c36c028aeee14b95901e1fb005c7201a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd63f0fa54284235bb73c59bf5cb2a92": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae3f1435c9714894810273ab6cc8fb39": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "188a4756346f45f796f49a44e694ad29": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8873103295c3485ca41a3347f8766a9d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f54ab7c292246db81dc7cbab21c7cbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e41117d4fed1445daf6fb85a01cbc81e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fefd7c010fdf4735aa9b2513db0497d0",
       "IPY_MODEL_f48e51c77737475fb4aaafce03380fb7",
       "IPY_MODEL_82778a1b41a0417b86fc1e29455fb383"
      ],
      "layout": "IPY_MODEL_5b6201ac09a64fa79f255ac5a14b3a5a"
     }
    },
    "fefd7c010fdf4735aa9b2513db0497d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f46a10202f08457393536590b5084b3e",
      "placeholder": "",
      "style": "IPY_MODEL_32dc1d47b7a545ff9759d3897eee59d2",
      "value": "tokenizer.model:100%"
     }
    },
    "f48e51c77737475fb4aaafce03380fb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12db66482d064d7e831767ccbcef1bec",
      "max": 499723,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ee332c51eee94dcbb5586f463e39afdf",
      "value": 499723
     }
    },
    "82778a1b41a0417b86fc1e29455fb383": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec1dd8303a3b4f5e97b3d218f3de8613",
      "placeholder": "",
      "style": "IPY_MODEL_09222bf385da4ce0be09da2e03b74f74",
      "value": "500k/500k[00:00&lt;00:00,7.61MB/s]"
     }
    },
    "5b6201ac09a64fa79f255ac5a14b3a5a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f46a10202f08457393536590b5084b3e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32dc1d47b7a545ff9759d3897eee59d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "12db66482d064d7e831767ccbcef1bec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee332c51eee94dcbb5586f463e39afdf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ec1dd8303a3b4f5e97b3d218f3de8613": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09222bf385da4ce0be09da2e03b74f74": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ba6646a5d5347059b14acdd79d6da07": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_756c21cc62c14083a6a6729f54232d79",
       "IPY_MODEL_0b2e8ac291734ae39059110f96493a18",
       "IPY_MODEL_cf27f80668ff40c9ac07a93197b34640"
      ],
      "layout": "IPY_MODEL_c65097f7cf4a4722a186cfb1822f0315"
     }
    },
    "756c21cc62c14083a6a6729f54232d79": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_beb1e66b6cc44e7490fb207bff6fbd2c",
      "placeholder": "",
      "style": "IPY_MODEL_3daf9a7b54124787b732fdb108089e00",
      "value": "tokenizer.json:100%"
     }
    },
    "0b2e8ac291734ae39059110f96493a18": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2adb1d17c4404860b49575ea15dbff13",
      "max": 1844408,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08718b87ba204a34a9449fc9a60f97ad",
      "value": 1844408
     }
    },
    "cf27f80668ff40c9ac07a93197b34640": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72a5c19cce364fed959f66f6521a3cf0",
      "placeholder": "",
      "style": "IPY_MODEL_57c3488609f046c8852641b73b83e678",
      "value": "1.84M/1.84M[00:00&lt;00:00,11.3MB/s]"
     }
    },
    "c65097f7cf4a4722a186cfb1822f0315": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "beb1e66b6cc44e7490fb207bff6fbd2c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3daf9a7b54124787b732fdb108089e00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2adb1d17c4404860b49575ea15dbff13": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08718b87ba204a34a9449fc9a60f97ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "72a5c19cce364fed959f66f6521a3cf0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57c3488609f046c8852641b73b83e678": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "00647dd5e8034abcb0217cc30e02177f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_57ef6e22ee014ce0b467d6ae328d37ec",
       "IPY_MODEL_26c3ff709f804abcacac8861c235b6f4",
       "IPY_MODEL_ae4d86bfa9914f6e81886ab56a011ec9"
      ],
      "layout": "IPY_MODEL_677fb8e2b421433fa412f9d75e319081"
     }
    },
    "57ef6e22ee014ce0b467d6ae328d37ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_156cc956117a4ed984446f3d1a3aebe3",
      "placeholder": "",
      "style": "IPY_MODEL_260bd2215a284268b2920be0c458393c",
      "value": "added_tokens.json:100%"
     }
    },
    "26c3ff709f804abcacac8861c235b6f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fadf515115e64ebeb234df1c3c9dce71",
      "max": 306,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20370596c8824147a0aab279ad6241d8",
      "value": 306
     }
    },
    "ae4d86bfa9914f6e81886ab56a011ec9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf8a3680974742c4962608eaac628a6e",
      "placeholder": "",
      "style": "IPY_MODEL_b9d3e3c74d644a27a6bfb7b374683f02",
      "value": "306/306[00:00&lt;00:00,28.4kB/s]"
     }
    },
    "677fb8e2b421433fa412f9d75e319081": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "156cc956117a4ed984446f3d1a3aebe3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "260bd2215a284268b2920be0c458393c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fadf515115e64ebeb234df1c3c9dce71": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20370596c8824147a0aab279ad6241d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bf8a3680974742c4962608eaac628a6e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9d3e3c74d644a27a6bfb7b374683f02": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6466f91527ee437ebd69ce5cb90bdf6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ecf37b3cecb8405685d622d1f3e05bcf",
       "IPY_MODEL_3277a976d1034d12b12ec767ae80e3c5",
       "IPY_MODEL_6a1bda5b60a1449caf1a7b1e9ef6e844"
      ],
      "layout": "IPY_MODEL_1fa0d48eb7624fc8806942941ca4b411"
     }
    },
    "ecf37b3cecb8405685d622d1f3e05bcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f17c35f573354ce78baf719054af4732",
      "placeholder": "",
      "style": "IPY_MODEL_6415e99c230c463298a92d761875fd2e",
      "value": "special_tokens_map.json:100%"
     }
    },
    "3277a976d1034d12b12ec767ae80e3c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f34f0115c644766a1c6cc5b61485608",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6044a91e47104cb9b18a8add2a0e4c6f",
      "value": 665
     }
    },
    "6a1bda5b60a1449caf1a7b1e9ef6e844": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d808e2efb7b54ae79e431f56ab9fc871",
      "placeholder": "",
      "style": "IPY_MODEL_6aa8bfe1c8864a679e8634806ec46dac",
      "value": "665/665[00:00&lt;00:00,53.1kB/s]"
     }
    },
    "1fa0d48eb7624fc8806942941ca4b411": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f17c35f573354ce78baf719054af4732": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6415e99c230c463298a92d761875fd2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f34f0115c644766a1c6cc5b61485608": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6044a91e47104cb9b18a8add2a0e4c6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d808e2efb7b54ae79e431f56ab9fc871": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6aa8bfe1c8864a679e8634806ec46dac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
